{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb468a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5d0e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported lazytransform v1.11. \n",
      "\n",
      "Imported featurewiz 0.5.3. Use the following syntax:\n",
      "    >>> wiz = FeatureWiz(feature_engg = '', nrows=None, transform_target=True, scalers=\"std\",\n",
      "        \t\tcategory_encoders=\"auto\", add_missing=False, verbose=0. imbalanced=False,\n",
      "        \t\tae_options={})\n",
      "    >>> X_train_selected, y_train = wiz.fit_transform(X_train, y_train)\n",
      "    >>> X_test_selected = wiz.transform(X_test)\n",
      "    >>> selected_features = wiz.features\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from featurewiz import LazyTransformer, SuloRegressor, SuloClassifier\n",
    "from featurewiz import FeatureWiz, cross_val_model_predictions, get_class_weights\n",
    "from featurewiz import print_regression_metrics, print_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2fc3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6366, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate_marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_married</th>\n",
       "      <th>children</th>\n",
       "      <th>religious</th>\n",
       "      <th>educ</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation_husb</th>\n",
       "      <th>affair_multiclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rate_marriage   age  yrs_married  children  religious  educ  occupation  \\\n",
       "0            3.0  32.0          9.0       3.0        3.0  17.0         2.0   \n",
       "1            3.0  27.0         13.0       3.0        1.0  14.0         3.0   \n",
       "2            4.0  22.0          2.5       0.0        1.0  16.0         3.0   \n",
       "3            4.0  37.0         16.5       4.0        3.0  16.0         5.0   \n",
       "4            5.0  27.0          9.0       1.0        1.0  14.0         3.0   \n",
       "\n",
       "   occupation_husb  affair_multiclass  \n",
       "0              5.0                  0  \n",
       "1              4.0                  3  \n",
       "2              5.0                  1  \n",
       "3              5.0                  0  \n",
       "4              4.0                  4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfile = 'c:/users/ram/documents/ram/data_sets/kaggle/diabetes.csv'\n",
    "datapath = '../Ram/Data_Sets/'\n",
    "filename = 'winequality.csv'\n",
    "filename = 'affairs.csv'\n",
    "trainfile = datapath+filename\n",
    "sep = ','\n",
    "dft = pd.read_csv(trainfile,sep=sep)\n",
    "dft.drop(['affair', 'affairs'],axis=1, inplace=True)\n",
    "print(dft.shape)\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5f9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'affair_multiclass'\n",
    "modeltype = 'Multi_Classification'\n",
    "preds = [x for x in list(dft) if x not in target]\n",
    "dft[target].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9451ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5092, 8) (1274, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from featurewiz import FE_kmeans_resampler\n",
    "if modeltype == 'Regression':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dft[preds], dft[target], test_size=0.20, random_state=1,)\n",
    "    #X_train_over, y_train_over = FE_kmeans_resampler(X_train, y_train, target, smote='',verbose=0)\n",
    "    #print(X_train_over.shape, X_test.shape)\n",
    "    #train, test = pd.concat([X_train_over, pd.Series(y_train_over,name=target)], axis=1), pd.concat([X_test, y_test], axis=1)\n",
    "    train, test = train_test_split(dft, test_size=0.20, random_state=42)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dft[preds], dft[target], test_size=0.20, \n",
    "                                                    stratify=dft[target],\n",
    "                                                    random_state=42)\n",
    "    train, test = train_test_split(dft, test_size=0.20, random_state=42,\n",
    "                                                    stratify=dft[target]\n",
    "                                                   )\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5d5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_target = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d86550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurewiz is given 0.8 as correlation limit...\n",
      "    Warning: Too many features will be generated since feature engg specified\n",
      "    final list of feature engineering given: ['interactions', 'poly2']\n",
      "    final list of category encoders given: ['onehot', 'label']\n",
      "Since Auto Encoders are selected for feature extraction,\n",
      "    SULOV is skipped...\n",
      "    Recursive XGBoost is skipped...\n",
      "DenoisingAutoEncoder()\n",
      "    AE dictionary given: dict_items([])\n",
      "    final list of scalers given: [minmax]\n",
      "Loaded input data. Shape = (5092, 8)\n",
      "    selecting 8 numeric features for further processing...\n",
      "#### Starting featurewiz transform for train data ####\n",
      "No groupby features created since no groupby feature engg specified\n",
      "No interactions created for categorical vars since number less than 2\n",
      "    Single_Label Multi_Classification problem \n",
      "Shape of dataset: (5092, 44). Now we classify variables into different types...\n",
      "    Returning dictionary for variable types with following keys:\n",
      "                        continuous_vars = 44, int_vars = 0, \n",
      "                        discrete_string_vars = 0, nlp_vars = 0,\n",
      "                        date_vars = 0, time_deltas = 0,\n",
      "                        categorical_vars = 0, date_zones = 0\n",
      "    no date time variables detected in this dataset\n",
      "    Beware! onehot encoding can create hundreds if not 1000s of variables...\n",
      "label encoder selected for transforming all categorical variables\n",
      "Using OneHotEncoder() and My_LabelEncoder() as encoders\n",
      "Caution: ### When you have categorical or date-time vars in data, scaling may not be helpful. ##\n",
      "Check the pipeline creation statement for errors (if any):\n",
      "\tmake_column_transformer((imp, intvars),(imp, floatvars),    remainder=remainder)\n",
      "    no other vars left in dataset to transform...\n",
      "Time taken to define data pipeline = 1 second(s)\n",
      "No model input given...\n",
      "Lazy Transformer Pipeline created...\n",
      "    transformed target from object type to numeric\n",
      "    Time taken to fit dataset = 1 second(s)\n",
      "    Time taken to transform dataset = 1 second(s)\n",
      "    Shape of transformed dataset: (5092, 44)\n",
      "Performing hyper param selection for DAE. Will take approx. 81 seconds\n",
      "    defining a pipeline with DAE\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Epoch 1/3\n",
      "255/255 [==============================] - 3s 6ms/step - loss: 0.2287 - mse: 0.2287 - val_loss: 0.0550 - val_mse: 0.0550\n",
      "Epoch 2/3\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 3/3\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0617 - mse: 0.0617 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "    time taken for DAE hyper param selection = 103 seconds\n",
      "{'feature_extractor__batch_size': 16, 'feature_extractor__encoding_dim': 5, 'feature_extractor__epochs': 3}\n",
      "Epoch 1/100\n",
      "255/255 [==============================] - 3s 6ms/step - loss: 0.2287 - mse: 0.2287 - val_loss: 0.0550 - val_mse: 0.0550\n",
      "Epoch 2/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0877 - mse: 0.0877 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 3/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0617 - mse: 0.0617 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 4/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 5/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 6/100\n",
      "255/255 [==============================] - 2s 6ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 7/100\n",
      "255/255 [==============================] - 1s 6ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 8/100\n",
      "255/255 [==============================] - 1s 6ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0294 - val_mse: 0.0294\n",
      "Epoch 9/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 10/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 11/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 12/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 13/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 14/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 15/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0231 - val_mse: 0.0231\n",
      "Epoch 16/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 17/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 18/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 19/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 20/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 21/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 22/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 23/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 24/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 25/100\n",
      "255/255 [==============================] - 2s 7ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 26/100\n",
      "255/255 [==============================] - 2s 8ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 27/100\n",
      "255/255 [==============================] - 2s 6ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 28/100\n",
      "255/255 [==============================] - 2s 7ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 29/100\n",
      "255/255 [==============================] - 2s 8ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 30/100\n",
      "255/255 [==============================] - 3s 12ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 31/100\n",
      "255/255 [==============================] - 3s 12ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 32/100\n",
      "255/255 [==============================] - 3s 11ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0201 - val_mse: 0.0201\n",
      "Epoch 33/100\n",
      "255/255 [==============================] - 2s 9ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 34/100\n",
      "255/255 [==============================] - 2s 7ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 35/100\n",
      "255/255 [==============================] - 2s 7ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 36/100\n",
      "255/255 [==============================] - 2s 7ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 37/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 38/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0193 - val_mse: 0.0193\n",
      "Epoch 39/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 40/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 42/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 43/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0184 - val_mse: 0.0184\n",
      "Epoch 44/100\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Fitting and transforming an Auto Encoder for dataset...\n",
      "Shape of transformed data due to auto encoder = (5092, 5)\n",
      "    Single_Label Multi_Classification problem \n",
      "    Dropping 1 columns due to zero variance: Index(['ae_feature_5'], dtype='object')\n",
      "    time taken to run entire featurewiz = 173 second(s)\n",
      "Recursive XGBoost selected 4 features...\n"
     ]
    }
   ],
   "source": [
    "fwiz = FeatureWiz(corr_limit=0.80, auto_encoders='dae', feature_engg=['interactions', 'poly2'], \n",
    "                  category_encoders='auto', scalers='MinMax',\n",
    "                 dask_xgboost_flag=False, nrows=None, verbose=2, skip_sulov=False,\n",
    "                 skip_xgboost=False, transform_target=transform_target,\n",
    "                 ae_options={}) \n",
    "X_train_selected, y_train = fwiz.fit_transform(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861d3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Starting featurewiz transform for test data ####\n",
      "Loaded input data. Shape = (1274, 8)\n",
      "    Beware! feature_engg will add 100s, if not 1000s of additional features to your dataset!\n",
      "#### Starting lazytransform for test data ####\n",
      "    Time taken to transform dataset = 1 second(s)\n",
      "    Shape of transformed dataset: (1274, 44)\n",
      "Shape of transformed data due to auto encoder = (1274, 5)\n",
      "    Dropping 1 columns due to zero variance: Index(['ae_feature_5'], dtype='object')\n",
      "Returning dataframe with 4 features \n"
     ]
    }
   ],
   "source": [
    "X_test_selected = fwiz.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515160fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transform_target:\n",
    "    y_test = fwiz.lazy.yformer.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92e34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bal accu 15%\n",
      "ROC AUC = 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88      1050\n",
      "           1       0.10      0.03      0.05        86\n",
      "           2       0.17      0.04      0.07        48\n",
      "           3       0.00      0.00      0.00        26\n",
      "           4       0.15      0.06      0.08        35\n",
      "           5       0.00      0.00      0.00        17\n",
      "           6       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.79      1274\n",
      "   macro avg       0.18      0.15      0.16      1274\n",
      "weighted avg       0.70      0.79      0.74      1274\n",
      "\n",
      "final average balanced accuracy score = 0.15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from featurewiz import get_class_distribution\n",
    "# Updating the Random Forest Classifier with the corrected class weights\n",
    "if modeltype == 'Regression':\n",
    "    rf_classifier = RandomForestRegressor(random_state=42)\n",
    "else:\n",
    "    # Correctly computing class weights for the classes present in the training set\n",
    "    class_weights_dict_corrected = get_class_distribution(y_train)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, class_weight=class_weights_dict_corrected, random_state=42)\n",
    "\n",
    "\n",
    "# Fitting the classifier on the training data\n",
    "rf_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = rf_classifier.predict(X_test_selected)\n",
    "\n",
    "if modeltype == 'Regression':\n",
    "    print_regression_metrics(y_test, y_pred, verbose=1)\n",
    "else:\n",
    "    # Evaluating the classifier\n",
    "    y_probas = rf_classifier.predict_proba(X_test_selected)\n",
    "    print_classification_metrics(y_test, y_pred, y_probas, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5f8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurewiz import IterativeDoubleClassifier,  IterativeSearchClassifier, IterativeBestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a20aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRFRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df41c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurewiz.blagging import BlaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7f15ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "import time\n",
    "start_time = time.time()\n",
    "early = BlaggingClassifier(n_estimators=200)\n",
    "late = RandomForestClassifier(n_estimators=100)\n",
    "base = XGBClassifier(n_estimators=200)\n",
    "#sdae =  IterativeBestClassifier(base_estimator=late)\n",
    "#sdae = IterativeSearchClassifier(base_estimator1=early, base_estimator2=late)\n",
    "sdae = IterativeDoubleClassifier(base_estimator1=early, base_estimator2=late, weights={1: 0.6, 2: 0.4} )\n",
    "sdae.fit(X_train_selected, y_train)\n",
    "print('Time taken = %0.0f seconds' %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccdc6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sdae.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e17e822",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bal accu 18%\n",
      "ROC AUC = 0.70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84      1050\n",
      "           1       0.10      0.09      0.10        86\n",
      "           2       0.08      0.08      0.08        48\n",
      "           3       0.07      0.12      0.09        26\n",
      "           4       0.06      0.06      0.06        35\n",
      "           5       0.06      0.06      0.06        17\n",
      "           6       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.70      1274\n",
      "   macro avg       0.17      0.18      0.17      1274\n",
      "weighted avg       0.71      0.70      0.70      1274\n",
      "\n",
      "final average balanced accuracy score = 0.18\n"
     ]
    }
   ],
   "source": [
    "if modeltype == 'Regression':\n",
    "    print_regression_metrics(y_test, y_pred, verbose=1)\n",
    "else:\n",
    "    # Evaluating the classifier\n",
    "    y_probas = sdae.predict_proba(X_test_selected)\n",
    "    print_classification_metrics(y_test, y_pred, y_probas, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597598c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
